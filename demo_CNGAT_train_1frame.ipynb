{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import gc\n",
    "\n",
    "        \n",
    "# load dataset\n",
    "testset = np.load('./testset_demo.npz')\n",
    "\n",
    "test_features = torch.from_numpy(testset['F']/100.0)## normalized dBZ：(lgz)/10\n",
    "test_lbs = torch.from_numpy( testset['L']) ## transformed precipitation labels：y=lg(y+1)\n",
    "test_mask = torch.from_numpy(testset['M'])\n",
    "del testset\n",
    "gc.collect()\n",
    "\n",
    "num_feats = test_features.shape[2]\n",
    "N_test  = test_features.shape[0]\n",
    "N_val = np.floor(N_test/2)\n",
    "N_val = N_val.astype(np.int32)\n",
    "\n",
    "trainset = np.load('./trainset_demo.npz') \n",
    "train_features = torch.from_numpy(trainset['F']/100.0)\n",
    "train_lbs = torch.from_numpy( (trainset['L']) )\n",
    "train_mask = torch.from_numpy(trainset['M'])\n",
    "del trainset\n",
    "gc.collect()\n",
    "\n",
    "N_train  = train_features.shape[0]\n",
    "\n",
    "indice = np.load('./rand_id_small.npz')\n",
    "rand_id = indice['I'] ##indice for validation and testing examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###preprocessing the data for training###\n",
    "\n",
    "test_labels=torch.zeros(test_lbs.shape)\n",
    "train_labels=torch.zeros(train_lbs.shape)\n",
    "k=1/6/np.log(10)\n",
    "b=np.log10(6)-k*5\n",
    "mask0=test_lbs<5\n",
    "mask1=test_lbs>=5\n",
    "C=torch.tensor([10.0]).float()\n",
    "\n",
    "test_labels[mask0] = torch.log(test_lbs[mask0].float()+1)/torch.log(C)## transformed precipitation labels：y=0.1lg(R+1)\n",
    "#test_labels[mask1] = (test_lbs[mask1]/9).float()\n",
    "test_labels[mask1] = (k*test_lbs[mask1]+b).float()\n",
    "\n",
    "mask0=train_lbs<5\n",
    "mask1=train_lbs>=5\n",
    "train_labels[mask0] = torch.log(train_lbs[mask0].float()+1)/torch.log(C)\n",
    "train_labels[mask1] = (k*train_lbs[mask1]+b).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "from dgl import DGLGraph\n",
    "from dgl.data import register_data_args, load_data\n",
    "#from gat import GAT\n",
    "from snat3 import SNAT3\n",
    "from utils import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def mkdir(path): \n",
    "    folder = os.path.exists(path)\n",
    "    if not folder:                   \n",
    "        os.makedirs(path)           \n",
    "\n",
    "def lerelu(x):\n",
    "    y=F.leaky_relu(x,negative_slope=0.1)\n",
    "    return y\n",
    "        \n",
    "def mean_square_error(y,labels):\n",
    "    mean_e= torch.sum((y-labels)**2)\n",
    "    n = y.size(0)\n",
    "    return mean_e, n\n",
    "\n",
    "def b_ms_error(y,labels):\n",
    "    s_e= torch.sum((labels+1)*(y-labels)**2)\n",
    "    n = (labels+1).sum()\n",
    "    return s_e, n\n",
    "\n",
    "def b_abs_error(y,labels):\n",
    "    s_e= torch.sum((labels+1)*abs(y-labels))\n",
    "    n = (labels+1).sum()\n",
    "    return s_e, n\n",
    "\n",
    "\n",
    "def evaluate0(model, features, labels, mask, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y = model(features)\n",
    "        y = y[mask]\n",
    "        y = y.flatten(0)\n",
    "        labels = labels[mask] \n",
    "        L = labels#torch.zeros(y.shape)\n",
    "        yy = y#torch.zeros(y.shape)\n",
    "        mask0 = y<np.log10(6)\n",
    "        mask1 = y>=np.log10(6)\n",
    "        yy[mask0] = 10**(y[mask0])-1\n",
    "        yy[mask1] = (y[mask1]-b)/k\n",
    "        \n",
    "        mask00 = labels<np.log10(6)\n",
    "        mask11 = labels>=np.log10(6)\n",
    "        L[mask00] = 10**(labels[mask00])-1\n",
    "        L[mask11] = (labels[mask11]-b)/k\n",
    "        return criterion(yy, L)\n",
    "\n",
    "\n",
    "gpu= 0 #gpu device\n",
    "#hyperparameters\n",
    "\n",
    "threhold = 0.35##\n",
    "num_heads=1\n",
    "num_out_heads=1\n",
    "num_layers = 6\n",
    "num_hidden= 16\n",
    "out_dim= 128\n",
    "early_stop= True\n",
    "fastmode= False\n",
    "\n",
    "if gpu < 0:\n",
    "    cuda = False\n",
    "else:\n",
    "    cuda = True\n",
    "    torch.cuda.set_device(gpu)\n",
    "\n",
    "from dgl.data.utils import load_graphs\n",
    "g = load_graphs(\"./grid800_600.bin\",[0])\n",
    "g = g[0][0]\n",
    "g = dgl.add_self_loop(g) \n",
    "if cuda:\n",
    "    g = g.to(torch.device('cuda:%01d'%(gpu)))\n",
    "\n",
    "\n",
    "# create model\n",
    "heads = torch.zeros(num_layers+1,dtype=torch.int32)\n",
    "nhidT = torch.zeros(num_layers+1,dtype=torch.int32)\n",
    "heads[0:num_layers]=num_heads\n",
    "heads[-1]=num_out_heads\n",
    "nhidT[0]= 16\n",
    "nhidT[1]=16\n",
    "nhidT[2]=16\n",
    "nhidT[3]=16\n",
    "nhidT[4]=16\n",
    "nhidT[5]=16\n",
    "nhidT[6]=16\n",
    "#nhidT[7]=16\n",
    "#nhidT[8]=16\n",
    "#nhidT[9]=16\n",
    "#nhidT[10]=16\n",
    "model = SNAT3(g,\n",
    "             threhold,\n",
    "            num_layers,\n",
    "            num_feats,\n",
    "            num_hidden,\n",
    "            out_dim,\n",
    "            heads=heads,\n",
    "            num_hiddenT=nhidT,\n",
    "            feat_drop =0,\n",
    "            attn_drop = 0,\n",
    "            negative_slope=0.2,\n",
    "            activation= torch.tanh,##\n",
    "            activationP= lerelu,\n",
    "            residual=False)\n",
    "\n",
    "\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "print(model)\n",
    "if early_stop:\n",
    "    stopper = EarlyStopping(patience=50)\n",
    "\n",
    "loss_fcn1 = torch.nn.MSELoss(reduction='none')   \n",
    "loss_fcn2 = torch.nn.L1Loss(reduction='none')   \n",
    "\n",
    "# use optimizer\n",
    "optimizer1 = torch.optim.Adam(\n",
    "    model.parameters(), lr=10e-6 )##\n",
    "#optimizer1 = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "dur = []\n",
    "for epoch in range(300):\n",
    "\n",
    "    val_error=0\n",
    "    loss_m = 0\n",
    "    N_total = 0\n",
    "    if epoch >= 3:\n",
    "        t0 = time.time()\n",
    "    rand_n=torch.randperm(N_train) \n",
    "    batchsize= 1  \n",
    "    nbatches = int(N_train/batchsize/2) #use 1/2 of the training data every epoch\n",
    "    N_nodes_train= 0\n",
    "    for batch in range(nbatches):\n",
    "        N_ba = 0\n",
    "        for n in range(batchsize): \n",
    "            model.train()\n",
    "\n",
    "            # forward\n",
    "            index_n = rand_n[n+batchsize*batch]\n",
    "            H0 = train_features[index_n][:,:].float()#.mean(dim=1).view(-1,1)\n",
    "            mean_H0 = H0.mean(dim=1)\n",
    "            #mean_H0 = H0\n",
    "            index_0rd = (mean_H0==0).flatten()\n",
    "            train_labels[index_n,index_0rd]=0##去除异常值\n",
    "            index_2rd = (train_lbs[index_n]>100).flatten() ##把R>100 的都去掉\n",
    "            train_mask[index_n,index_2rd]=False ##去除太大的zhi\n",
    "            mask = train_mask[index_n].bool()\n",
    "            labels = train_labels[index_n,mask].float()\n",
    "            lbs = train_lbs[index_n,mask]\n",
    "            if cuda== True:\n",
    "                H0=H0.cuda()\n",
    "                labels=labels.cuda()\n",
    "                lbs = lbs.cuda()\n",
    "            N_t = labels.size(0)\n",
    "            y = model(H0) \n",
    "            index_0 = (labels) ==0 ###标签为0的索引\n",
    "            W_nodes = lbs + 1#10**(labels)  ##  weigtht for none zero label: W\n",
    "            #W_nodes[index_0]=0.01  ## weight for zero label: W0\n",
    "            loss1 = loss_fcn1( y[mask,:].flatten(),\n",
    "                            labels.float() )  ###compute the loss for every (labeled) node(MSE)\n",
    "            loss2 = loss_fcn2( y[mask,:].flatten(),\n",
    "                            labels.float() )  ###compute the loss for every (labeled) node(MAE)\n",
    "            loss = loss1*W_nodes*0.1+ loss2*W_nodes\n",
    "            loss_w = loss.mean() ## average loss among all nodes\n",
    "            loss_m += loss.sum().item()\n",
    "            loss_w.backward()\n",
    "            N_ba +=N_t\n",
    "        optimizer1.step() \n",
    "        optimizer1.zero_grad()\n",
    "        N_nodes_train += N_ba           \n",
    "    for n in range(N_val):\n",
    "        model.eval()\n",
    "        H0 = test_features[rand_id[n]][:,:].float()#.mean(dim=1).view(-1,1)\n",
    "        mean_H0 = H0.mean(dim=1)\n",
    "        index_0rd = (mean_H0==0).flatten()\n",
    "        test_labels[rand_id[n],index_0rd]=0##去除异常值\n",
    "        mask = (test_labels[rand_id[n]])>0\n",
    "        index_2rd = (test_lbs[rand_id[n]]>100).flatten()\n",
    "        test_mask[rand_id[n],index_2rd]=False ##去除太大的zhi\n",
    "        mask = mask&test_mask[rand_id[n]].bool()\n",
    "        test_F= H0\n",
    "        test_L=test_labels[rand_id[n]].float()\n",
    "        if cuda==True:\n",
    "            test_F=test_F.cuda()\n",
    "            test_L=test_L.cuda()\n",
    "        val_error0, N0 = evaluate0(model, test_F, test_L, mask, b_abs_error)#\n",
    "        val_error += val_error0\n",
    "        N_total += N0\n",
    "        \n",
    "    val_error/=N_total\n",
    "    \n",
    "    path1='D:\\\\CNGATs\\\\CNGAT_parameters'\n",
    "    mkdir(path1) \n",
    "    if early_stop: \n",
    "        if stopper.step(-val_error, model, path1+'\\\\demoCNGAT3_16_6_128_tanh_1frame_newgrid_wL1L2_01adam1e-5_th35_BMAE_MixedOutNewKB.pt'):   \n",
    "            break\n",
    "    loss_m = loss_m/N_nodes_train\n",
    "    if epoch >= 3:\n",
    "        dur.append(time.time() - t0)\n",
    "    print(\"Epoch {:05d} | Time(s) {:.4f} | Loss {:.6f} |val_error{:.4f}\".\n",
    "              format(epoch, np.mean(dur), loss_m, val_error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PPXXpytorch] *",
   "language": "python",
   "name": "conda-env-PPXXpytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
